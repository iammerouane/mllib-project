{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1add4a5",
   "metadata": {},
   "source": [
    "# Mini-projet Machine learning avec MLLIB\n",
    "\n",
    "Le problème de prédiction de l'attrition des employés consiste à prédire si un employé quittera ou non son poste. Cela peut être utile pour les entreprises afin de prendre des mesures préventives pour retenir leurs employés précieux et réduire le taux d'attrition. L'attrition peut être coûteuse pour les entreprises en termes de recrutement et de formation de nouveaux employés, ainsi que de la perte de connaissances et d'expertise. L'utilisation de Spark MLLIB permet de construire des modèles prédictifs à grande échelle pour résoudre ce problème. En utilisant des techniques telles que l'encodage des caractéristiques, la visualisation des données et la construction de modèles Random Forests et Gradient Boosting, les data scientists peuvent développer des modèles prédictifs précis qui peuvent aider les entreprises à réduire le taux d'attrition et à améliorer la rétention des employés.\n",
    "\n",
    "# 1. Dataset :*Attrition et performance des employés IBM HR Analytics Prévoyez l'attrition de vos précieux employés\n",
    "Il s'agit d'un ensemble de données fictif créé par les data scientists d'IBM. Nous devons explorer l'ensemble de données, comprendre les algorithmes et les techniques qui peuvent y être appliqués. Nous essaierons d'obtenir des informations significatives à partir de l'ensemble de données, comme quels sont les facteurs qui ont un impact sur l'attrition des employés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c82f0",
   "metadata": {},
   "source": [
    "# 2. Travail demandé :\n",
    "\n",
    "# 1. Importer les données : Tout d'abord, il est nécessaire d'importer les données dans un format exploitable par Spark MLLIB. Les données peuvent être stockées dans des fichiers CSV, des bases de données ou des formats de fichiers parquet, entre autres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder, MinMaxScaler, ChiSqSelector\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63dfce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'application Spark avec le nom PAE - Prédiction Attrition Employés - Merouane Bennaceur V1.0\n",
    "spark = SparkSession.builder.appName(\"PAE - Prédiction Attrition Employés - Merouane Bennaceur V1.0\").getOrCreate()\n",
    "\n",
    "# Charger les données\n",
    "data = spark.read.csv(\"HR-Employee-Attrition.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd2588",
   "metadata": {},
   "source": [
    "# 2. Explorer les données : Une fois les données importées, il est important d'en explorer la structure et les caractéristiques. Cela peut inclure l'analyse des valeurs manquantes, des distributions de variables et de l'existence de corrélations entre les variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84945e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 10 premières lignes\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage des 10 premières lignes\")\n",
    "data.show(10)\n",
    "\n",
    "# Afficher le schéma\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage du schéma\")\n",
    "data.printSchema()\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n\\n\")\n",
    "print(\"Statistiques descriptives\")\n",
    "data.describe().show()\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "data_summary = data.describe().toPandas()\n",
    "print(data_summary)\n",
    "\n",
    "# Vérification des valeurs manquantes\n",
    "# Identifiecation des colonnes avec des valeurs manquantes\n",
    "print(\"\\n\\n\")\n",
    "print(\"Valeurs manquantes par colonnes\")\n",
    "data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()\n",
    "\n",
    "\n",
    "# Analyse des distributions de variables\n",
    "# Exemple avec age\n",
    "print(\"\\n\\n\")\n",
    "print(\"Histogramme de distribution de la variable age\")\n",
    "col_name = \"age\"\n",
    "col_data = data.select(col_name).toPandas()\n",
    "sns.histplot(col_data[col_name], kde=True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Examiner les corrélations entre les variables et visualiser les relations\n",
    "# Création d'un vecteur avec les colonnes numériques\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=[\"Age\", \"DailyRate\"], outputCol=vector_col)\n",
    "data_vector = assembler.transform(data).select(vector_col)\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "matrix = Correlation.corr(data_vector, vector_col).collect()[0][0]\n",
    "correlation_matrix = matrix.toArray().tolist()\n",
    "print(\"\\n\\n\")\n",
    "print(\"Matrice de corrélation entre Age et DailyRate\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Exemple pour un nuage de points entre deux variables numériques (Age, DailyRate)\n",
    "x_col = \"Age\"\n",
    "y_col = \"DailyRate\"\n",
    "scatter_data = data.select(x_col, y_col).toPandas()\n",
    "sns.scatterplot(data=scatter_data, x=x_col, y=y_col)\n",
    "print(\"\\n\\n\")\n",
    "print(\"Nuage de points de corrélation entre Age et DailyRate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28f86d",
   "metadata": {},
   "source": [
    "# 3. Nettoyer les données : Après l'exploration des données, il est possible que certaines données soient incohérentes, mal formatées ou contiennent des valeurs aberrantes. Dans ce cas, il est important de nettoyer les données avant de les utiliser pour entraîner un modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f41b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va supprimer les valeurs aberrantes les colonnes numériques en utilisant l'intervalle interquartile (IQR).\n",
    "\n",
    "# Liste des colonnes numériques\n",
    "numerical_columns = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]\n",
    "\n",
    "data_cleaned = data\n",
    "\n",
    "for column in numerical_columns:\n",
    "    Q1 = data.approxQuantile(column, [0.25], 0.05)[0]\n",
    "    Q3 = data.approxQuantile(column, [0.75], 0.05)[0]\n",
    "    IQR = Q3 - Q1\n",
    "    lower_range = Q1 - 1.5 * IQR\n",
    "    upper_range = Q3 + 1.5 * IQR\n",
    "\n",
    "    data_cleaned = data_cleaned.filter((col(column) >= lower_range) & (col(column) <= upper_range))\n",
    "\n",
    "# Affichage dataset nettoyé\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage dataset nettoyé\")\n",
    "data_cleaned.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c5699f",
   "metadata": {},
   "source": [
    "# 4. Préparer les données : Selon le modèle que vous souhaitez utiliser, vous devrez peut-être préparer les données pour le rendre compatible avec ce modèle. Cela peut inclure la normalisation des variables, la transformation des variables catégorielles en variables numériques et la séparation des données en ensembles de formation et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation de toutes les colonnes numériques et transformation des colonnes discrettes en variables numériques (one-hot encoding).\n",
    "\n",
    "# Liste des colonnes discrétes\n",
    "categorical_columns = [\"BusinessTravel\", \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\"]\n",
    "\n",
    "# Stages pour le pipeline\n",
    "stages = []\n",
    "\n",
    "# One-hot encoding pour les colonnes catégorielles\n",
    "for column in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "    encoder = OneHotEncoder(inputCol=f\"{column}Index\", outputCol=f\"{column}Vec\")\n",
    "    stages += [indexer, encoder]\n",
    "\n",
    "# VectorAssembler et MinMaxScaler pour les colonnes numériques\n",
    "for column in numerical_columns:\n",
    "    assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}Vec\")\n",
    "    scaler = MinMaxScaler(inputCol=f\"{column}Vec\", outputCol=f\"{column}Scaled\")\n",
    "    stages += [assembler, scaler]\n",
    "\n",
    "# Créer et appliquer un pipeline\n",
    "pipeline = Pipeline(stages=stages)\n",
    "data_prepared = pipeline.fit(data_cleaned).transform(data_cleaned)\n",
    "\n",
    "# Affichage dataset préparé\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage dataset préparé\")\n",
    "data_prepared.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc64b4",
   "metadata": {},
   "source": [
    "# 5. Sélectionner les fonctionnalités : Si les données contiennent de nombreuses variables, vous pouvez envisager de sélectionner les fonctionnalités qui sont les plus pertinentes pour votre modèle. Cela peut aider à améliorer la précision de votre modèle et à réduire le temps nécessaire pour l'entraîner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecb726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilisation méthode Chi-Squared pour sélectionner les fonctionnalités les plus pertinentes pour le modèle. \n",
    "# pour sélectionner les 10 fonctionnalités les + importantes parmi les colonnes discrétes et numériques prétraitées.\n",
    "\n",
    "# Combinez toutes les colonnes d'entités en un seul vecteur\n",
    "input_cols = [f\"{col}Scaled\" for col in numerical_columns] + [f\"{col}Vec\" for col in categorical_columns]\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "data_features = assembler.transform(data_prepared)\n",
    "\n",
    "# Indexez la colonne Attrition (cible)\n",
    "indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "data_features = indexer.fit(data_features).transform(data_features)\n",
    "\n",
    "# Utilisation ChiSqSelector pour sélectionner les fonctionnalités les plus pertinentes\n",
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "selected_data = selector.fit(data_features).transform(data_features)\n",
    "selected_data = selected_data.select(\"features\", \"selectedFeatures\", \"label\")\n",
    "\n",
    "# Affichage dataset avec les fonctionnalités pertinentes\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage dataset avec les fonctionnalités pertinentes\")\n",
    "selected_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb21527",
   "metadata": {},
   "source": [
    "# 6. Équilibrer les données : Si les données contiennent un déséquilibre entre les classes, il peut être nécessaire d'équilibrer les données en utilisant des techniques telles que SMOTE (Synthetic Minority Over-sampling Technique) pour augmenter le nombre d'échantillons de la classe minoritaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculez le taux d'équilibrage\n",
    "attrition_counts = selected_data.groupBy(\"label\").count().collect()\n",
    "minority_count = min(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "majority_count = max(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "balancing_ratio = minority_count / majority_count\n",
    "\n",
    "# Équilibrez les données\n",
    "majority_label = max(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "minority_label = min(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "\n",
    "majority_data = selected_data.filter(col(\"label\") == majority_label)\n",
    "minority_data = selected_data.filter(col(\"label\") == minority_label)\n",
    "\n",
    "majority_data_downsampled = majority_data.sample(withReplacement=False, fraction=balancing_ratio, seed=42)\n",
    "balanced_data = majority_data_downsampled.union(minority_data)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Affichage dataset équilibré\")\n",
    "balanced_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a1a278",
   "metadata": {},
   "source": [
    "# 7. Diviser les données : Une fois que les données sont nettoyées et préparées, il est important de diviser les données en ensembles de formation et de test. L'ensemble de formation est utilisé pour entraîner le modèle, tandis que l'ensemble de test est utilisé pour évaluer les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0926abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = balanced_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80519d98",
   "metadata": {},
   "source": [
    "# 8. Entraîner le modèle : Utilisez l'ensemble de formation pour entraîner le modèle en utilisant l'algorithme approprié. Les algorithmes populaires pour la classification incluent Random Forest, Gradient Boosting, Logistic Regression et Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"selectedFeatures\", labelCol=\"label\", numTrees=100)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "\n",
    "# Gradient Boosting\n",
    "gbt = GBTClassifier(featuresCol=\"selectedFeatures\", labelCol=\"label\", maxIter=100)\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "\n",
    "# Logistic Regression:\n",
    "lr = LogisticRegression(featuresCol=\"selectedFeatures\", labelCol=\"label\", maxIter=100)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Decision Tree:\n",
    "dt = DecisionTreeClassifier(featuresCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "dt_model = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83f8cc",
   "metadata": {},
   "source": [
    "# 9. Évaluer le modèle : Une fois que le modèle est entraîné, utilisez l'ensemble de test pour évaluer ses performances. Les métriques courantes pour évaluer les performances d'un modèle de classification incluent la précision, le rappel, le score F1 et la courbe ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c0cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour évaluer les modèles\n",
    "def evaluate_model(model, test_data):\n",
    "    predictions = model.transform(test_data)\n",
    "    evaluator = BinaryClassificationEvaluator()\n",
    "    roc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "    predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd.map(lambda p: (float(p[0]), float(p[1])))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1_score = metrics.fMeasure(1.0)\n",
    "    return roc, precision, recall, f1_score\n",
    "\n",
    "# Évaluation des modèles\n",
    "models = [(\"Random Forest\", rf_model), (\"Gradient Boosting\", gbt_model), (\"Logistic Regression\", lr_model), (\"Decision Tree\", dt_model)]\n",
    "\n",
    "for model_name, model in models:\n",
    "    roc, precision, recall, f1_score = evaluate_model(model, test_data)\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Précision: {precision}\")\n",
    "    print(f\"  Rappel: {recall}\")\n",
    "    print(f\"  Score F1: {f1_score}\")\n",
    "    print(f\"  ROC: {roc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bfdb0",
   "metadata": {},
   "source": [
    "# 10. Optimiser le modèle : Si le modèle ne répond pas aux attentes, il est possible que les paramètres du modèle doivent être optimisés pour obtenir de meilleures performances. Cela peut inclure l'ajustement des hyperparamètres, la modification de l'algorithme ou la sélection de différentes fonctionnalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a01563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour optimiser les modèles on peut utiliser la validation croisée avec un ensemble de paramètres à tester. \n",
    "# Par exemple avec la régression logistique car c'est elle qui a obtenu les meilleurs évaluations.\n",
    "# On peut faire l'optimisation du modèle avec validation croisée et recherche sur grille.\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001]).addGrid(lr.fitIntercept, [True, False]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "cross_val = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=BinaryClassificationEvaluator(), numFolds=5)\n",
    "cv_model = cross_val.fit(train_data)\n",
    "best_lr_model = cv_model.bestModel\n",
    "\n",
    "# Évaluer le modèle optimisé\n",
    "roc, precision, recall, f1_score = evaluate_model(best_lr_model, test_data)\n",
    "print(\"Logistic Regression (Optimisé):\")\n",
    "print(f\"  Précision: {precision}\")\n",
    "print(f\"  Rappel: {recall}\")\n",
    "print(f\"  Score F1: {f1_score}\")\n",
    "print(f\"  ROC: {roc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2820ed84",
   "metadata": {},
   "source": [
    "# 11. Appliquer le modèle : Une fois que le modèle est entraîné et testé, il peut être appliqué aux nouvelles données pour faire des prédictions sur la probabilité d'attrition d'un employé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filename):\n",
    "    data = spark.read.csv(filename, header=True, inferSchema=True)\n",
    "    data_cleaned = data\n",
    "    numerical_columns = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]\n",
    "    categorical_columns = [\"BusinessTravel\", \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\"]\n",
    "    for column in numerical_columns:\n",
    "        Q1 = data.approxQuantile(column, [0.25], 0.05)[0]\n",
    "        Q3 = data.approxQuantile(column, [0.75], 0.05)[0]\n",
    "        IQR = Q3 - Q1\n",
    "        lower_range = Q1 - 1.5 * IQR\n",
    "        upper_range = Q3 + 1.5 * IQR\n",
    "        data_cleaned = data_cleaned.filter((col(column) >= lower_range) & (col(column) <= upper_range))\n",
    "    stages = []\n",
    "    for column in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "        encoder = OneHotEncoder(inputCol=f\"{column}Index\", outputCol=f\"{column}Vec\")\n",
    "        stages += [indexer, encoder]\n",
    "    for column in numerical_columns:\n",
    "        assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}Vec\")\n",
    "        scaler = MinMaxScaler(inputCol=f\"{column}Vec\", outputCol=f\"{column}Scaled\")\n",
    "        stages += [assembler, scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    data_prepared = pipeline.fit(data_cleaned).transform(data_cleaned)\n",
    "    input_cols = [f\"{col}Scaled\" for col in numerical_columns] + [f\"{col}Vec\" for col in categorical_columns]\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "    data_features = assembler.transform(data_prepared)\n",
    "    indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "    data_features = indexer.fit(data_features).transform(data_features)\n",
    "    selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "    selected_data = selector.fit(data_features).transform(data_features)\n",
    "    attrition_counts = selected_data.groupBy(\"label\").count().collect()\n",
    "    minority_count = min(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "    majority_count = max(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "    balancing_ratio = minority_count / majority_count\n",
    "    majority_label = max(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "    minority_label = min(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "    majority_data = selected_data.filter(col(\"label\") == majority_label)\n",
    "    minority_data = selected_data.filter(col(\"label\") == minority_label)\n",
    "    majority_data_downsampled = majority_data.sample(withReplacement=False, fraction=balancing_ratio, seed=42)\n",
    "    balanced_data = majority_data_downsampled.union(minority_data)\n",
    "\n",
    "    return balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d29ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = preprocess_data(\"New-Employee-Data.csv\")\n",
    "new_data.show()\n",
    "\n",
    "new_data_features = new_data\n",
    "\n",
    "predictions = best_lr_model.transform(new_data_features)\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da008f39",
   "metadata": {},
   "source": [
    "#  11.2 tester avec un input (ligne) sous forme de texte en entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dcf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Attrition\", StringType(), True),\n",
    "    StructField(\"BusinessTravel\", StringType(), True),\n",
    "    StructField(\"DailyRate\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"DistanceFromHome\", IntegerType(), True),\n",
    "    StructField(\"Education\", IntegerType(), True),\n",
    "    StructField(\"EducationField\", StringType(), True),\n",
    "    StructField(\"EmployeeCount\", IntegerType(), True),\n",
    "    StructField(\"EmployeeNumber\", IntegerType(), True),\n",
    "    StructField(\"EnvironmentSatisfaction\", IntegerType(), True),\n",
    "    StructField(\"Gender\", StringType(), True),\n",
    "    StructField(\"HourlyRate\", IntegerType(), True),\n",
    "    StructField(\"JobInvolvement\", IntegerType(), True),\n",
    "    StructField(\"JobLevel\", IntegerType(), True),\n",
    "    StructField(\"JobRole\", StringType(), True),\n",
    "    StructField(\"JobSatisfaction\", IntegerType(), True),\n",
    "    StructField(\"MaritalStatus\", StringType(), True),\n",
    "    StructField(\"MonthlyIncome\", IntegerType(), True),\n",
    "    StructField(\"MonthlyRate\", IntegerType(), True),\n",
    "    StructField(\"NumCompaniesWorked\", IntegerType(), True),\n",
    "    StructField(\"Over18\", StringType(), True),\n",
    "    StructField(\"OverTime\", StringType(), True),\n",
    "    StructField(\"PercentSalaryHike\", IntegerType(), True),\n",
    "    StructField(\"PerformanceRating\", IntegerType(), True),\n",
    "    StructField(\"RelationshipSatisfaction\", IntegerType(), True),\n",
    "    StructField(\"StandardHours\", IntegerType(), True),\n",
    "    StructField(\"StockOptionLevel\", IntegerType(), True),\n",
    "    StructField(\"TotalWorkingYears\", IntegerType(), True),\n",
    "    StructField(\"TrainingTimesLastYear\", IntegerType(), True),\n",
    "    StructField(\"WorkLifeBalance\", IntegerType(), True),\n",
    "    StructField(\"YearsAtCompany\", IntegerType(), True),\n",
    "    StructField(\"YearsInCurrentRole\", IntegerType(), True),\n",
    "    StructField(\"YearsSinceLastPromotion\", IntegerType(), True),\n",
    "    StructField(\"YearsWithCurrManager\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "def preprocess_data(filename):\n",
    "    data = spark.read.csv(filename, header=True, inferSchema=True)\n",
    "    data_cleaned = data\n",
    "    numerical_columns = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]\n",
    "    categorical_columns = [\"BusinessTravel\", \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\"]\n",
    "    for column in numerical_columns:\n",
    "        Q1 = data.approxQuantile(column, [0.25], 0.05)[0]\n",
    "        Q3 = data.approxQuantile(column, [0.75], 0.05)[0]\n",
    "        IQR = Q3 - Q1\n",
    "        lower_range = Q1 - 1.5 * IQR\n",
    "        upper_range = Q3 + 1.5 * IQR\n",
    "        data_cleaned = data_cleaned.filter((col(column) >= lower_range) & (col(column) <= upper_range))\n",
    "    stages = []\n",
    "    for column in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "        encoder = OneHotEncoder(inputCol=f\"{column}Index\", outputCol=f\"{column}Vec\")\n",
    "        stages += [indexer, encoder]\n",
    "    for column in numerical_columns:\n",
    "        assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}Vec\")\n",
    "        scaler = MinMaxScaler(inputCol=f\"{column}Vec\", outputCol=f\"{column}Scaled\")\n",
    "        stages += [assembler, scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    data_prepared = pipeline.fit(data_cleaned).transform(data_cleaned)\n",
    "    input_cols = [f\"{col}Scaled\" for col in numerical_columns] + [f\"{col}Vec\" for col in categorical_columns]\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "    data_features = assembler.transform(data_prepared)\n",
    "    indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "    data_features = indexer.fit(data_features).transform(data_features)\n",
    "    selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "    selected_data = selector.fit(data_features).transform(data_features)\n",
    "    attrition_counts = selected_data.groupBy(\"label\").count().collect()\n",
    "    minority_count = min(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "    majority_count = max(attrition_counts, key=lambda x: x[\"count\"])[\"count\"]\n",
    "    balancing_ratio = minority_count / majority_count\n",
    "    majority_label = max(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "    minority_label = min(attrition_counts, key=lambda x: x[\"count\"])[\"label\"]\n",
    "    majority_data = selected_data.filter(col(\"label\") == majority_label)\n",
    "    minority_data = selected_data.filter(col(\"label\") == minority_label)\n",
    "    majority_data_downsampled = majority_data.sample(withReplacement=False, fraction=balancing_ratio, seed=42)\n",
    "    balanced_data = majority_data_downsampled.union(minority_data)\n",
    "\n",
    "    return balanced_data, majority_data_downsampled, minority_data\n",
    "\n",
    "def preprocess_input(data, majority_data_downsampled, minority_data):\n",
    "    data_cleaned = data\n",
    "    numerical_columns = [\"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]\n",
    "    categorical_columns = [\"BusinessTravel\", \"Department\", \"EducationField\", \"Gender\", \"JobRole\", \"MaritalStatus\", \"OverTime\"]\n",
    "    for column in numerical_columns:\n",
    "        Q1 = data.approxQuantile(column, [0.25], 0.05)[0]\n",
    "        Q3 = data.approxQuantile(column, [0.75], 0.05)[0]\n",
    "        IQR = Q3 - Q1\n",
    "        lower_range = Q1 - 1.5 * IQR\n",
    "        upper_range = Q3 + 1.5 * IQR\n",
    "        data_cleaned = data_cleaned.filter((col(column) >= lower_range) & (col(column) <= upper_range))\n",
    "    stages = []\n",
    "    for column in categorical_columns:\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=f\"{column}Index\")\n",
    "        assembler = VectorAssembler(inputCols=[f\"{column}Index\"], outputCol=f\"{column}Vec\")\n",
    "        stages += [indexer, assembler]\n",
    "    for column in numerical_columns:\n",
    "        assembler = VectorAssembler(inputCols=[column], outputCol=f\"{column}Vec\")\n",
    "        scaler = MinMaxScaler(inputCol=f\"{column}Vec\", outputCol=f\"{column}Scaled\")\n",
    "        stages += [assembler, scaler]\n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    data_prepared = pipeline.fit(data_cleaned).transform(data_cleaned)\n",
    "    input_cols = [f\"{col}Scaled\" for col in numerical_columns] + [f\"{col}Vec\" for col in categorical_columns]\n",
    "    assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "    data_features = assembler.transform(data_prepared)\n",
    "    indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "    data_features = indexer.fit(data_features).transform(data_features)\n",
    "    selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "    selected_data = selector.fit(data_features).transform(data_features)\n",
    "    balanced_data = majority_data_downsampled.union(minority_data)\n",
    "    return balanced_data\n",
    "\n",
    "\n",
    "balanced_data, majority_data_downsampled, minority_data = preprocess_data(\"HR-Employee-Attrition.csv\")\n",
    "\n",
    "\n",
    "# Un input en entrée\n",
    "input_str = \"34,No,Travel_Rarely,628,Research & Development,8,3,Medical,1,2068,2,Male,82,4,2,Laboratory Technician,3,Married,4404,10228,2,Y,No,12,3,1,80,0,6,3,4,4,3,1,2\"\n",
    "input_list = input_str.split(\",\")\n",
    "input_list = [int(x) if x.isdigit() else x for x in input_list]\n",
    "\n",
    "# créer un dataframe Spark à partir de l'input\n",
    "input_df = spark.createDataFrame([input_list], schema=schema)\n",
    "\n",
    "# Préparer les données d'entrée en utilisant le pipeline\n",
    "input_features = preprocess_input(input_df, majority_data_downsampled, minority_data)\n",
    "\n",
    "# Appliquer le modèle optimisé (par exemple, best_lr_model) aux données d'entrée\n",
    "prediction = best_lr_model.transform(input_features)\n",
    "\n",
    "result = prediction.select(\"prediction\", \"probability\").first().asDict()\n",
    "print(\"Résultat de la prédiciton (0.0 pour No Attrition et 1.0 pour Yes Attrition): \" + str(result[\"prediction\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
